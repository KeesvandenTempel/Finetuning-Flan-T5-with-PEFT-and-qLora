{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455c625e",
   "metadata": {},
   "source": [
    "<H2><a href=\"https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805#3e72\">Cuda installatie 11.8</a></h2>\n",
    "\n",
    "<table border=\"1\">\n",
    "    <tr>\n",
    "        <th>Component</th>\n",
    "        <th>Description</th>\n",
    "        <th>Latest Version (as of April 2023)</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>NVIDIA RTX A2000 Laptop GPU</td>\n",
    "        <td>A graphics processing unit designed for deep learning and AI tasks.</td>\n",
    "        <td>N/A</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>NVIDIA Graphics Drivers</td>\n",
    "        <td>Software that allows the operating system and programs to use your NVIDIA graphics hardware.</td>\n",
    "        <td><a href=\"https://www.nvidia.com/download/driverResults.aspx/175200/en-us/\">Driver release 460: Latest NVIDIA RTX A2000 driver</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>CUDA Toolkit</td>\n",
    "        <td>A development environment for creating high performance GPU-accelerated applications.</td>\n",
    "        <td><a href=\"https://developer.nvidia.com/cuda-11-8-0-download-archive?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_local\">11.8</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>cuDNN Library for Windows</td>\n",
    "        <td>A GPU-accelerated library for deep neural networks.</td>\n",
    "        <td><a href=\"https://developer.nvidia.com/rdp/cudnn-archive\">cuDNN v8.9.6 (November 1st, 2023), for CUDA 11.x</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>PyTorch Library (CUDA-Compatible Version)</td>\n",
    "    <td>An open-source machine learning library with GPU acceleration support, tailored for CUDA (e.g., CUDA 11.8). <font color=\"red\">hier gaat het mis: pip-installatie instructie werkt niet</font></td>\n",
    "    <td><a href=\"https://pytorch.org/get-started/previous-versions/\">CUDA 11.8 (e.g., torch+cu116)</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Transformers Library (Hugging Face)</td>\n",
    "        <td>A state-of-the-art natural language processing library for training and deploying transformers models.</td>\n",
    "        <td><a href=\"https://github.com/huggingface/transformers\">4.35.2</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>BitsAndBytes Library</td>\n",
    "        <td>A library for efficient training of large-scale neural networks, allowing reduced precision and memory-efficient optimization.</td>\n",
    "        <td><a href=\"https://pypi.org/project/bitsandbytes/\">0.26.0</a></td>\n",
    "    </tr>\n",
    "    <!-- Add additional components here -->\n",
    "</table>\n",
    "\n",
    "<img src=\"variables.png\" width=\"50%\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ededf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA CUDA versie 12.2\n",
    "# bitsandbytes==0.40.3 (from versions: 0.31.8, 0.32.0, 0.32.1, 0.32.2, 0.32.3, 0.33.0, 0.33.1, 0.34.0, 0.35.0, 0.35.1, \n",
    "# 0.35.2, 0.35.3, 0.35.4, 0.36.0, 0.36.0.post1, 0.36.0.post2, 0.37.0, 0.37.1, 0.37.2, 0.38.0, 0.38.0.post1, \n",
    "# 0.38.0.post2, 0.38.1, 0.39.0, 0.39.1, 0.40.0, 0.40.0.post1, 0.40.0.post2, 0.40.0.post3, 0.40.0.post4, 0.40.1, \n",
    "# 0.40.1.post1, 0.40.2, 0.41.0, 0.41.1, 0.41.2, 0.41.2.post1, 0.41.2.post2)\n",
    "\n",
    "# https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# https://github.com/TimDettmers/bitsandbytes\n",
    "# https://medium.com/geekculture/install-cuda-and-cudnn-on-windows-linux-52d1501a8805#3e72\n",
    "\n",
    "# Gedistribueerde WSL2 - versie\n",
    "# https://github.com/bigscience-workshop/petals\n",
    "\n",
    "# Wouter Versies\n",
    "# %pip install torch==2.0.1\n",
    "# %pip install bitsandbytes==0.41.0\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install ipywidgets\n",
    "%pip install --upgrade pandas\n",
    "%pip install --upgrade datetime\n",
    "%pip install --upgrade matplotlib\n",
    "%pip install --upgrade scikit-learn\n",
    "\n",
    "# CUDA 11.8 versies\n",
    "%pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install --upgrade torchdata==0.7.1\n",
    "%pip install --upgrade SentencePiece==0.1.99\n",
    "%pip install --upgrade datasets==2.15.0\n",
    "%pip install --upgrade evaluate==0.4.1\n",
    "%pip install --upgrade rouge_score==0.1.2\n",
    "\n",
    "%pip install --upgrade transformers==4.35.2\n",
    "%pip install accelerate==0.25.0.\n",
    "%pip install --upgrade loralib==0.1.2\n",
    "%pip install --upgrade peft==0.6.2\n",
    "%pip install wandb==0.16.1 \n",
    "%pip install git+https://github.com/Keith-Hon/bitsandbytes-windows.git\n",
    "\n",
    "# CUDA 12.3 versies: doesn't work well....\n",
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#%pip install torchdata==0.7.1\n",
    "#%pip install SentencePiece==0.1.99\n",
    "#%pip install datasets==2.15.0\n",
    "#%pip install evaluate==0.4.1\n",
    "#%pip install rouge_score==0.1.2\n",
    "\n",
    "#%pip install transformers==4.36.1\n",
    "#%pip install accelerate==0.25.0\n",
    "#%pip install loralib==0.1.2\n",
    "#%pip install peft==0.7.1\n",
    "#%pip install wandb==0.16.1 \n",
    "#%pip install bitsandbytes==0.41.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9b917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.11.5 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:26:23) [MSC v.1916 64 bit (AMD64)]\n",
      "PyTorch version:  2.1.0+cu118\n",
      "Torchvision version:  0.16.0+cu118\n",
      "CUDA version:  11.8\n",
      "cuDNN version:  8700\n",
      "GPU is available\n",
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "\n",
    "# cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "\n",
    "# Number of GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(\"Python version: \", sys.version)\n",
    "print(\"PyTorch version: \", pytorch_version)\n",
    "print(\"Torchvision version: \", torchvision.__version__)\n",
    "print(\"CUDA version: \", cuda_version)\n",
    "print(\"cuDNN version: \", cudnn_version)\n",
    "print(\"GPU is\", \"available\" if cuda_available else \"NOT AVAILABLE\")\n",
    "print(\"Num GPUs:\", num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aace288f-064d-4cac-9772-6c22c158f4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++ DEBUG INFORMATION +++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "++++++++++ POTENTIALLY LIBRARY-PATH-LIKE ENV VARS ++++++++++\n",
      "'MPLBACKEND': 'module://matplotlib_inline.backend_inline'\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "WARNING: Please be sure to sanitize sensible info from any such env vars!\n",
      "\n",
      "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
      "COMPILED_WITH_CUDA = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\__main__.py\", line 47, in <module>\n",
      "    print(f\"COMPUTE_CAPABILITIES_PER_GPU = {get_compute_capabilities(cuda)}\")\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\main.py\", line 336, in get_compute_capabilities\n",
      "    check_cuda_result(cuda, cuda.cuDeviceGetCount(ct.byref(nGpus)))\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'cuDeviceGetCount'\n"
     ]
    }
   ],
   "source": [
    "# Onderstaande instructie geeft een foutmelding, maar dit heeft geen invloed op het script.\n",
    "\n",
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa716f4f-2124-4296-bb9a-07a769c6b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "Every_record_count = 2\n",
    "AI_Labs_dataset = True\n",
    "Use_LLama = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60772d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Use_LLama == True:\n",
    "    from llama import BasicModelRunner\n",
    "\n",
    "    non_finetuned = BasicModelRunner(\"google/flan-t5-base\")\n",
    "    non_finetuned_output = non_finetuned(\"Tell me how to train my dog to sit\")\n",
    "    print(non_finetuned_output)\n",
    "\n",
    "    from llama import BasicModelRunner\n",
    "\n",
    "    model = BasicModelRunner(\"EleutherAI/pythia-410m\") \n",
    "    model.load_data_from_jsonlines(\"lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n",
    "    model.train(is_public=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0073336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary C:\\Users\\kvdte\\anaconda3\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data collators are objects that will form a batch by using a list of dataset elements as input. \n",
    "# These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
    "# To be able to build batches, data collators may apply some processing (like padding). \n",
    "# Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) \n",
    "# on the formed batch.\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# The Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. \n",
    "# It’s used in most of the example scripts.\n",
    "# Before instantiating your Trainer, create a TrainingArguments to access all the points of customization during training.\n",
    "# The API supports distributed training on multiple GPUs/TPUs, mixed precision through NVIDIA Apex and Native AMP for PyTorch.\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model \n",
    "# you are supplying to the from_pretrained() method. AutoClasses are here to do this job for you so \n",
    "# that you automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.\n",
    "# Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# AutoModelForSeq2SeqLM is used for language models with encoder-decoder architecture like T5 and BART, \n",
    "# while AutoModelForCausalLM is used for auto-regressive language models like all the GPT models.\n",
    "# These two classes are conceptual APIs to automatically infer a specific model class for the two types of models\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# By using load_in_4bit=True when calling the .from_pretrained method, you can divide your memory use by 4\n",
    "# You can load a model by roughly halving the memory requirements by using load_in_8bit=True argument when calling .from_pretrained method\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, GenerationConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12a2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "data_file_path = \"./data/PenD/NL_ProdenDienstenSum.xlsx\"\n",
    "df = pd.read_excel(data_file_path)\n",
    "# Rename the columns\n",
    "df.rename(columns={'Uitleg': 'document', 'Samenvatting': 'summary'}, inplace=True)\n",
    "\n",
    "# subsample the recordset\n",
    "df = df.iloc[::Every_record_count]\n",
    "\n",
    "# Split the data into training and tempory sets\n",
    "df_train, df_temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the temporary set into validation and test sets (50-50 split)\n",
    "# This will result in 80% training, 10% validation, and 10% test.\n",
    "df_eval, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7728bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2878ccab8b384cdda8f88616cf81424d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb074c8f596a4902ada267cd1d8a361f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec9855bff274391a0a68900e0729334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "huggingface_dataset_name = \"yhavinga/xsum_dutch\"\n",
    "ydataset = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "ydataset = ydataset.filter(lambda example, index: index % Every_record_count == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728737b",
   "metadata": {},
   "source": [
    "Suppose we have a dictionary that looks like this: {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, 'Hallo': 3, 'wereld': 4, '!': 5}\n",
    "\n",
    "And we want to convert the text \"Hallo wereld!\" into input_ids. We would do the following:\n",
    "\n",
    "Tokenization: [\"Hallo\", \"wereld\", \"!\"] Numeric Representation: [3, 4, 5]\n",
    "\n",
    "The attention_mask is a list of the same length as the input_ids list, where each element in the attention_mask is either a 1 or a 0.\n",
    "\n",
    "For example, if the input_ids for a sentence are \"[3, 4, 5, 0, 0]\" (where \"0\" is a padding token), the corresponding attention_mask would be \"[1, 1, 1, 0, 0]\".\n",
    "\n",
    "The max_length argument in the tokenizer function specifies the maximum length of the token list that is returned. It is a way to ensure that all text input processed by the model has a uniform length.\n",
    "\n",
    "Here are a few things that max_length can affect:\n",
    "\n",
    "Truncation: If the token list generated from the input text is longer than max_length, the extra tokens will be truncated to meet the maximum length.\n",
    "\n",
    "Padding: If the token list is shorter than max_length, padding tokens are added to ensure that the list has the specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfbcd7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maak een samenvatting van de volgende tekst.\n",
      "\n",
      "----\n",
      "Dit is een tekst die samengevat moet worden\n",
      "----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Makeprompt(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Maak een samenvatting van de volgende tekst.\n",
    "\n",
    "----\n",
    "{dialogue}\n",
    "----\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "print (Makeprompt(\"Dit is een tekst die samengevat moet worden\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e9f09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieer een aangepaste dataset voor tekstsamenvatting\n",
    "\n",
    "# de TextSummarizationDataset klasse erft van Dataset, maar definieert zijn eigen __init__ methode \n",
    "# die drie parameters accepteert. Wanneer je een instantie van TextSummarizationDataset creëert, \n",
    "# roep je deze aangepaste __init__ methode aan.\n",
    "\n",
    "\n",
    "class TextSummarizationDataframe(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        #Initialisatie van de TextSummarizationDataset klasse.\n",
    "        # Parameters:\n",
    "        #- dataframe (pd.DataFrame): Een Pandas DataFrame dat de dataset bevat. Het moet twee kolommen hebben: 'Uitleg' voor de volledige tekst en 'Samenvatting' voor de samengevatte tekst.\n",
    "        #- tokenizer (transformers.PreTrainedTokenizer): Een tokenizer object om de tekst om te zetten naar token-ids. Dit object komt uit de Hugging Face Transformers bibliotheek.\n",
    "        #- max_length (int): De maximale lengte van de token-ids lijsten na het tokenizen. Teksten die langer zijn, worden afgekapt.\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        #Geeft het aantal items in de dataset terug.\n",
    "        #Returns: #int: Het aantal items in de DataFrame.\n",
    "\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #Haalt een enkel datapunt op uit de dataset.\n",
    "        #Parameters: index (int): De index van het op te halen datapunt.\n",
    "        #Returns: dict: Een woordenboek met drie sleutels: 'input_ids', 'attention_mask', en 'labels'. Elk van deze sleutels heeft als waarde een afgevlakte tensor van token-ids.\n",
    "        row = self.dataframe.iloc[index]\n",
    "        \n",
    "        # Haal de tekst en samenvatting op uit de geselecteerde rij\n",
    "        text = row['document']\n",
    "        summary = row['summary']\n",
    "        \n",
    "        # Voeg de prefix toe aan de tekst\n",
    "        text_with_prefix = Makeprompt(text)\n",
    "        \n",
    "        # Tokenize de tekst en samenvatting met de opgegeven tokenizer en maximale lengte\n",
    "        inputs = self.tokenizer(text_with_prefix, max_length=self.max_length, truncation=True, return_tensors='pt', padding='max_length')\n",
    "        targets = self.tokenizer(summary, max_length=self.max_length, truncation=True, return_tensors='pt', padding='max_length')\n",
    "        \n",
    "        # Geef het resultaat terug als een dictionary\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }\n",
    "    \n",
    "\n",
    "class TextSummarizationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Selecteer het datapunt uit de dataset op basis van de index\n",
    "        data = self.dataset[index]\n",
    "        \n",
    "        # Haal de tekst en samenvatting op uit het geselecteerde datapunt\n",
    "        text = data['document']\n",
    "        summary = data['summary']\n",
    "        \n",
    "        # Voeg de prefix toe aan de tekst\n",
    "        text_with_prefix = Makeprompt(text)\n",
    "        \n",
    "        # Tokenize de tekst en samenvatting met de opgegeven tokenizer en maximale lengte\n",
    "        inputs = self.tokenizer(text_with_prefix, max_length=self.max_length, truncation=True, return_tensors='pt', padding='max_length')\n",
    "        targets = self.tokenizer(summary, max_length=self.max_length, truncation=True, return_tensors='pt', padding='max_length')\n",
    "        \n",
    "        # Geef het resultaat terug als een dictionary\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten()\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77b4df",
   "metadata": {},
   "source": [
    "<img src=float.png width=50% align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "177c0d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA introduces a number of innovations to save memory without sacrificing performance: \n",
    "#       (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights \n",
    "#       (b) double quantization to reduce the average memory footprint by quantizing the quantization constants\n",
    "#       (c) paged optimizers to manage memory spikes.# BitsandBytes configuratie bestand maken: Bits andBytes is een wrapper om de cuda implementatie: matrix multiplicatie, etc\n",
    "\n",
    "# Very good explanation of quantization en Lora in Youtube video: https://youtu.be/TPcXVJ1VSRI?si=bzQcCTK4JjDrKGWi\n",
    "# Also look at OASST1 dataset: OpenAssistant/oasst1\n",
    "# And look at https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "# And: https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996\n",
    "\n",
    "# Colab notebook: https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing#scrollTo=ZwXZbQ2dSwzI\n",
    "# https://huggingface.co/blog/hf-bitsandbytes-integration\n",
    "\n",
    "# Enable loading the model in 8-bit quantization: kan alleen op windows met cuda 11.8\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  \n",
    "    bnb_8bit_use_double_quant=True,\n",
    "    bnb_8bit_quant_type=\"nf4\",\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  \n",
    ")\n",
    "\n",
    "if 1==2:\n",
    "    # Enable loading the model in 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "                  load_in_4bit=True,                     # Het model wordt in 4 bits geladen, dus 16 niveaus (van -7 tot +8). Geheugengebruik is een factor 4 minder\n",
    "                  bnb_4bit_use_double_quant=True,        # Er wordt een dubbele quantization toegepast: scheelt extra 0.4 bits per parameter.\n",
    "                  bnb_4bit_quant_type=\"nf4\",             # Ook bekend als NVIDIA's \"FastFloat4\" (FF4) maakt gebruik op een GPU mogelijk\n",
    "                  bnb_4bit_compute_dtype=torch.bfloat16, # I.h.g.v. inference wordt dequantization toegepast tot bfloat16\n",
    "                  llm_int8_enable_fp32_cpu_offload=True  # Enable CPU offloading for FP32 if there's insufficient GPU memory\n",
    "                  )\n",
    "\n",
    "# Als er te weinig geheugen op de GPU is wordt er een gedeelte door de CPU gedaan\n",
    "quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "\n",
    "device_map = {\n",
    "    \"transformer.word_embeddings\": 0,\n",
    "    \"transformer.word_embeddings_layernorm\": 0,\n",
    "    \"lm_head\": \"cpu\",\n",
    "    \"transformer.h\": 0,\n",
    "    \"transformer.ln_f\": 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "490c558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Het original model is 990311424 bytes groot\n"
     ]
    }
   ],
   "source": [
    "# Define the model and tokenizer\n",
    "# Documentatie over de T5-modellen: https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "# model_name = \"google/flan-t5-small\"    #7,6 mio parameters\n",
    "model_name = \"google/flan-t5-base\"     #247 mio parameters\n",
    "# model_name = \"flax-community/t5-base-dutch\" #222 mio parameters\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"flax-community/t5-base-dutch\")\n",
    "# model_name = \"google/flan-t5-large\"    #783 mio parameters, 3,3 GB\n",
    "\n",
    "#tokenizer = T5Tokenizer.from_pretrained(model_name, device_map=\"auto\")\n",
    "#original_model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "#finetune_model = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\")\n",
    "#peft_model = T5ForConditionalGeneration.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "finetune_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "peft_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "print(\"Het original model is \" + str(original_model.get_memory_footprint()) + \" bytes groot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d5ed024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes van de datasets:\n",
      "Training: 624\n",
      "Test: 79\n",
      "Validation: 78\n"
     ]
    }
   ],
   "source": [
    "# Bereid de dataset en data loader voor\n",
    "# Hier moet je de `df` DataFrame en `tokenizer` object definiëren voordat je deze code uitvoert.\n",
    "# Bereid de datasets en data loaders voor de training en evaluatie sets\n",
    "if (AI_Labs_dataset == True):\n",
    "    trainingset = TextSummarizationDataframe(df_train, tokenizer, max_length=512)\n",
    "    testset = TextSummarizationDataframe(df_test, tokenizer, max_length=512)\n",
    "    evalset = TextSummarizationDataframe(df_eval, tokenizer, max_length=512)\n",
    "else:\n",
    "    trainingset = TextSummarizationDataset(ydataset['train'], tokenizer, max_length=512)\n",
    "    testset = TextSummarizationDataset(ydataset['test'], tokenizer, max_length=512)\n",
    "    evalset = TextSummarizationDataset(ydataset['validation'], tokenizer, max_length=512)\n",
    "\n",
    "train_data_loader = DataLoader(trainingset, batch_size=4, shuffle=True)\n",
    "test_data_loader = DataLoader(testset, batch_size=4, shuffle=False)  # Shuffle is meestal niet nodig voor testset\n",
    "eval_data_loader = DataLoader(evalset, batch_size=4, shuffle=False)  # Shuffle is meestal niet nodig voor evaluatieset\n",
    "\n",
    "print(f\"Shapes van de datasets:\")\n",
    "print(f\"Training: {len(trainingset)}\")\n",
    "print(f\"Test: {len(testset)}\")\n",
    "print(f\"Validation: {len(evalset)}\")\n",
    "\n",
    "#print(df.head())\n",
    "#print(\"\\n\\n\")\n",
    "#print(trainingset[0])  #hier wordt de interne functie __getitem__ aangeroepen om record 0 te tonen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bd8194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model_name):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in original_model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b2e602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:\n",
      "Maak een samenvatting van de volgende tekst. ---- Wanneer u als bedrijf uit Nederland een aanvraag wilt doen voor een kentekenschorsing bij de Nederlandse Rijksoverheid, kunt u dit doen op basis van Artikel 50 van het Kentekenreglement. Een kentekenschorsing houdt in dat het kenteken van een voertuig tijdelijk wordt stopgezet, waardoor het voertuig niet op de openbare weg mag worden gebruikt. Dit kan bijvoorbeeld handig zijn wanneer u een voertuig voor langere tijd niet gebruikt, bijvoorbeeld tijdens een periode van stilstand of wanneer het voertuig wordt gerepareerd. Om een kentekenschorsing aan te vragen, dient u een online aanvraagformulier in te vullen op onze website. Hierbij dient u enkele gegevens in te vullen, zoals het kenteken van het voertuig, uw contactgegevens en de gewenste periode van schorsing. Na het indienen van de aanvraag ontvangt u een bevestiging per e-mail. De kosten voor een kentekenschorsing bedragen €73,10 per jaar. Na betaling van deze kosten wordt de schorsing van kracht. Het is belangrijk om te weten dat een kenteken\n",
      "\n",
      "Baseline menselijke samenvatting:\n",
      "Vraag een kentekenschorsing aan bij de Nederlandse Rijksoverheid op basis van Artikel 50 Kentekenreglement. Kosten bedragen €73,10 per jaar en de schorsing is maximaal 1 jaar geldig.\n",
      "\n",
      "Model gegenereerde samenvatting:\n",
      "De kentekenschorsing mijn te voorbeeld om het voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdelijk voor een voertuig tijdel\n"
     ]
    }
   ],
   "source": [
    "def ShowModelResult(model_to_eval, tokenizer_to_eval, trainingset_to_eval):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_to_eval = model_to_eval.to(device)\n",
    "\n",
    "    index = 2\n",
    "    datapoint = trainingset_to_eval[index]\n",
    "\n",
    "    dialoog = datapoint['input_ids'].to(device)\n",
    "    samenvatting = datapoint['labels'].to(device)\n",
    "\n",
    "    dialoog_text = tokenizer_to_eval.decode(dialoog, skip_special_tokens=True)\n",
    "    samenvatting_text = tokenizer_to_eval.decode(samenvatting, skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer_to_eval(dialoog_text, return_tensors='pt').to(device)\n",
    "    output = model_to_eval.generate(input_ids=inputs[\"input_ids\"], max_length=200)\n",
    "    output_text = tokenizer_to_eval.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    ###@@@ WAT GEBEURD ER ALS DE TEKST DIE SAMENGEVAT MOET WORDEN TE LANG IS ?\n",
    "    \n",
    "    print(\"Document:\")\n",
    "    print(dialoog_text)\n",
    "    print(\"\\nBaseline menselijke samenvatting:\")\n",
    "    print(samenvatting_text)\n",
    "    print(\"\\nModel gegenereerde samenvatting:\")\n",
    "    print(output_text)\n",
    "\n",
    "ShowModelResult(original_model, tokenizer, trainingset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda4d64",
   "metadata": {},
   "source": [
    "<h1>DataLoader (PyTorch):</h1>\n",
    "\n",
    "- A component of the PyTorch library specifically designed for loading data for training machine learning models.\n",
    "- It allows for easy batching, shuffling, and loading data in parallel using multiple workers.\n",
    "- Typically used in combination with a Dataset class, where the data can be preprocessed and transformed into a format suitable for machine learning models.\n",
    "- It's part of a machine learning framework, which is different from the general-purpose data manipulation provided by NumPy and pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73a5b22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda GPU aanwezig: True\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the Flan-T5 model\n",
    "num_epochs = 8\n",
    "max_steps = 1000\n",
    "max_length = 512\n",
    "    \n",
    "# Check if CUDA is available and set the device\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Cuda GPU aanwezig: \" + str(cuda))\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "# Assume that `original_model` is already defined and is your T5 model\n",
    "finetune_model = finetune_model\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(finetune_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Since T5 is not a masked language model, we set this to False\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    disable_tqdm=False,            # Disable progress bars\n",
    "    output_dir='./results',        # Directory for saving model and checkpoints\n",
    "    overwrite_output_dir=True,     # Overwrite the content of the output directory\n",
    "    evaluation_strategy='steps',   # Evaluate at the end of each epoch; kan de waarde 'epoch' of 'steps' hebben\n",
    "    num_train_epochs=num_epochs,   # Set the number of epochs: kan ook waarde 'steps' hebben\n",
    "    per_device_train_batch_size=10, # Batch size per device during training\n",
    "    save_steps=500,                # After # steps model is saved \n",
    "    per_device_eval_batch_size=10, # Batch size for evaluation\n",
    "    logging_dir='./logs',          # Directory for storing logs\n",
    "    logging_steps=5,               # Log every # steps\n",
    "    gradient_accumulation_steps = 4,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adafactor\",\n",
    "    \n",
    "    # Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,            # Only last # models are saved. Older ones are deleted.\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee94b231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint 0.990311424 GB\n",
      "Flops 2739.029409792 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  original_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, max_length)\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "# print(original_model)\n",
    "print(\"Memory footprint\", original_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe5e0e",
   "metadata": {},
   "source": [
    "<img src=\"taskmanager.png\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd09db5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkvdtempel\u001b[0m (\u001b[33mailabs\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\kvdte\\My Drive (kvdtempel@gmail.com)\\AILabs\\JUP_LLM\\wandb\\run-20231216_092314-k6e7i0t0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ailabs/huggingface/runs/k6e7i0t0' target=\"_blank\">cerulean-terrain-27</a></strong> to <a href='https://wandb.ai/ailabs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ailabs/huggingface' target=\"_blank\">https://wandb.ai/ailabs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ailabs/huggingface/runs/k6e7i0t0' target=\"_blank\">https://wandb.ai/ailabs/huggingface/runs/k6e7i0t0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 6:14:30, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.447000</td>\n",
       "      <td>0.025292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.078400</td>\n",
       "      <td>0.007560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.003282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.034600</td>\n",
       "      <td>0.002127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.001579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.001330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.020900</td>\n",
       "      <td>0.001166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>0.001043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>0.000952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.014400</td>\n",
       "      <td>0.000880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.000706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>0.000707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.000690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.000682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.000660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.000627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.000612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.000606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.000604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=120, training_loss=0.036771966920544706, metrics={'train_runtime': 22660.0665, 'train_samples_per_second': 0.22, 'train_steps_per_second': 0.005, 'total_flos': 3258075482947584.0, 'train_loss': 0.036771966920544706, 'epoch': 7.62})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=finetune_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=trainingset,   # Pass the training dataset\n",
    "    eval_dataset=evalset,        # Pass the evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce70e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./trained_model_flan-t5v1')\n",
    "# tokenizer.save_pretrained('./trained_model_flan-t5v1')  GEEFT FOUTMELDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e173de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:\n",
      "Maak een samenvatting van de volgende tekst. ---- Wanneer u als bedrijf uit Nederland een aanvraag wilt doen voor een kentekenschorsing bij de Nederlandse Rijksoverheid, kunt u dit doen op basis van Artikel 50 van het Kentekenreglement. Een kentekenschorsing houdt in dat het kenteken van een voertuig tijdelijk wordt stopgezet, waardoor het voertuig niet op de openbare weg mag worden gebruikt. Dit kan bijvoorbeeld handig zijn wanneer u een voertuig voor langere tijd niet gebruikt, bijvoorbeeld tijdens een periode van stilstand of wanneer het voertuig wordt gerepareerd. Om een kentekenschorsing aan te vragen, dient u een online aanvraagformulier in te vullen op onze website. Hierbij dient u enkele gegevens in te vullen, zoals het kenteken van het voertuig, uw contactgegevens en de gewenste periode van schorsing. Na het indienen van de aanvraag ontvangt u een bevestiging per e-mail. De kosten voor een kentekenschorsing bedragen €73,10 per jaar. Na betaling van deze kosten wordt de schorsing van kracht. Het is belangrijk om te weten dat een kenteken\n",
      "\n",
      "Baseline menselijke samenvatting:\n",
      "Vraag een kentekenschorsing aan bij de Nederlandse Rijksoverheid op basis van Artikel 50 Kentekenreglement. Kosten bedragen €73,10 per jaar en de schorsing is maximaal 1 jaar geldig.\n",
      "\n",
      "Model gegenereerde samenvatting:\n",
      "Maak een samenvatting van de volgende tekst. ---- Wanneer u als bedrijf uit Nederland een aanvraag wilt doen voor een kentekenschorsing bij de Nederlandse Rijksoverheid, kunt u dit doen op basis van Artikel 50 van het Kentekenreglement. Een kentekenschorsing houdt in dat het kenteken van een voertuig tijdelijk wordt stopgezet, waardoor het voertuig niet op de openbare weg mag worden gebruikt. Dit kan bijvoorbeeld hand\n"
     ]
    }
   ],
   "source": [
    "ShowModelResult(finetune_model, tokenizer, trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab8bad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "pad_token = tokenizer.pad_token\n",
    "\n",
    "print (pad_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b541c5e",
   "metadata": {},
   "source": [
    "<h1>Evalueer het Model Kwantitatief (met de ROUGE Metric)</h1>\n",
    "De ROUGE-metriek helpt bij het kwantificeren van de geldigheid van door modellen geproduceerde samenvattingen. Het vergelijkt samenvattingen met een \"basislijn\" samenvatting die meestal door een mens is gemaakt. Hoewel het niet perfect is, geeft het wel een indicatie van de algehele toename in effectiviteit van samenvatten die we hebben bereikt door finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55269b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb104635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>finetune_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Om een sportactiviteitensubsidie aan te vragen...</td>\n",
       "      <td>De aanvraag kan om een tijd om een aanvraag om...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Als u in Nederland woont en lesgeld heeft beta...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vraag een vrijstelling aan voor een onderwijsa...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vraag een persoonsgebonden budget (PGB) aan bi...</td>\n",
       "      <td>PGB. ----</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Een bedrijf kan een afvalstortontheffing aanvr...</td>\n",
       "      <td>The province will be afvalstortontheffing on t...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Als een bedrijf uit Nederland zijn verplichtin...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Voor een verlenging van een duur prestatiebeur...</td>\n",
       "      <td>Maak een samenvatting van de volgende text. --...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Om een energiesubsidie aan te vragen bij uw ge...</td>\n",
       "      <td>Dutch will be able to make a decision on the a...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Voor het aanvragen van een monumentensubsidie ...</td>\n",
       "      <td>---- Um een subsidie aan te vinden voor een mo...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maak een melding van een luchtvaart voorvalmel...</td>\n",
       "      <td>De voorval voorval voorval voorval voorval voo...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Om een sportactiviteitensubsidie aan te vragen...   \n",
       "1  Als u in Nederland woont en lesgeld heeft beta...   \n",
       "2  Vraag een vrijstelling aan voor een onderwijsa...   \n",
       "3  Vraag een persoonsgebonden budget (PGB) aan bi...   \n",
       "4  Een bedrijf kan een afvalstortontheffing aanvr...   \n",
       "5  Als een bedrijf uit Nederland zijn verplichtin...   \n",
       "6  Voor een verlenging van een duur prestatiebeur...   \n",
       "7  Om een energiesubsidie aan te vragen bij uw ge...   \n",
       "8  Voor het aanvragen van een monumentensubsidie ...   \n",
       "9  Maak een melding van een luchtvaart voorvalmel...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  De aanvraag kan om een tijd om een aanvraag om...   \n",
       "1  De het het het het het het het het het het het...   \n",
       "2  De het het het het het het het het het het het...   \n",
       "3                                          PGB. ----   \n",
       "4  The province will be afvalstortontheffing on t...   \n",
       "5  De het het het het het het het het het het het...   \n",
       "6  Maak een samenvatting van de volgende text. --...   \n",
       "7  Dutch will be able to make a decision on the a...   \n",
       "8  ---- Um een subsidie aan te vinden voor een mo...   \n",
       "9  De voorval voorval voorval voorval voorval voo...   \n",
       "\n",
       "                            finetune_model_summaries  \n",
       "0  Maak een samenvatting van de volgende tekst. -...  \n",
       "1  Maak een samenvatting van de volgende tekst. -...  \n",
       "2  Maak een samenvatting van de volgende tekst. -...  \n",
       "3  Maak een samenvatting van de volgende tekst. -...  \n",
       "4  Maak een samenvatting van de volgende tekst. -...  \n",
       "5  Maak een samenvatting van de volgende tekst. -...  \n",
       "6  Maak een samenvatting van de volgende tekst. -...  \n",
       "7  Maak een samenvatting van de volgende tekst. -...  \n",
       "8  Maak een samenvatting van de volgende tekst. -...  \n",
       "9  Maak een samenvatting van de volgende tekst. -...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the device (GPU if available, else CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move models to the chosen device\n",
    "original_model = original_model.to(device)\n",
    "finetune_model = finetune_model.to(device)\n",
    "\n",
    "if (AI_Labs_dataset == False):\n",
    "    dialogues = evalset.dataset['document'][:10]\n",
    "    human_baseline_summaries = evalset.dataset['summary'][:10]\n",
    "else:\n",
    "    dialogues = evalset.dataframe['document'][:10]\n",
    "    human_baseline_summaries = evalset.dataframe['summary'][:10]\n",
    "\n",
    "original_model_summaries = []\n",
    "finetune_model_summaries = []\n",
    "\n",
    "# Loop for generating summaries\n",
    "for _, dialogue in enumerate(dialogues):\n",
    "    prompt = Makeprompt(dialogue)\n",
    "\n",
    "    # Tokenize the prompt and move input_ids to the device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids.to(device)\n",
    "\n",
    "    # Generate summaries with the original model\n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, max_length=200)\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "\n",
    "    # Generate summaries with the finetuned model\n",
    "    finetune_model_outputs = finetune_model.generate(input_ids=input_ids, max_length=200)\n",
    "    finetune_model_text_output = tokenizer.decode(finetune_model_outputs[0], skip_special_tokens=True)\n",
    "    finetune_model_summaries.append(finetune_model_text_output)\n",
    "\n",
    "# Combine and display the summaries\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, finetune_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'finetune_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc460ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(original_model_summaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "002c6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.1522105957661788, 'rouge2': 0.04607785902721354, 'rougeL': 0.10551464308540717, 'rougeLsum': 0.10651960098156801}\n",
      "FINETUNE MODEL:\n",
      "{'rouge1': 0.4854847435313273, 'rouge2': 0.29797759145874136, 'rougeL': 0.3721755845667165, 'rougeLsum': 0.3713568735187307}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = list(human_baseline_summaries)\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetune_model_results = rouge.compute(\n",
    "    predictions=finetune_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(finetune_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print({\n",
    "    'rouge1': original_model_results['rouge1'],\n",
    "    'rouge2': original_model_results['rouge2'],\n",
    "    'rougeL': original_model_results['rougeL'],\n",
    "    'rougeLsum': original_model_results['rougeLsum'],\n",
    "})\n",
    "\n",
    "print('FINETUNE MODEL:')\n",
    "print({\n",
    "    'rouge1': finetune_model_results['rouge1'],\n",
    "    'rouge2': finetune_model_results['rouge2'],\n",
    "    'rougeL': finetune_model_results['rougeL'],\n",
    "    'rougeLsum': finetune_model_results['rougeLsum'],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e61b671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of FINETUNE MODEL over ORIGINAL MODEL\n",
      "rouge1: 218.96%\n",
      "rouge2: 546.68%\n",
      "rougeL: 252.72%\n",
      "rougeLsum: 248.63%\n"
     ]
    }
   ],
   "source": [
    "print(\"Absolute percentage improvement of FINETUNE MODEL over ORIGINAL MODEL\")\n",
    "\n",
    "# Extract the mid fmeasure values for both models\n",
    "original_scores = {\n",
    "    'rouge1': original_model_results['rouge1'],\n",
    "    'rouge2': original_model_results['rouge2'],\n",
    "    'rougeL': original_model_results['rougeL'],\n",
    "    'rougeLsum': original_model_results['rougeLsum'],\n",
    "}\n",
    "\n",
    "finetune_scores = {\n",
    "    'rouge1': finetune_model_results['rouge1'],\n",
    "    'rouge2': finetune_model_results['rouge2'],\n",
    "    'rougeL': finetune_model_results['rougeL'],\n",
    "    'rougeLsum': finetune_model_results['rougeLsum'],\n",
    "}\n",
    "\n",
    "# Calculate the percentage improvement for each score\n",
    "for key in original_scores:\n",
    "    original_score = original_scores[key]\n",
    "    finetune_score = finetune_scores[key]\n",
    "    improvement = (finetune_score - original_score) / original_score * 100\n",
    "    print(f'{key}: {improvement:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770af9c7",
   "metadata": {},
   "source": [
    "<h1>Uitvoeren van Parameter Efficiënte Finetuning (PEFT)</h1>\n",
    "Laten we nu Parameter Efficiënte Finetuning (PEFT) uitvoeren in tegenstelling tot \"volledige Finetuning\" zoals je hierboven hebt gedaan. PEFT is een vorm van instructieFinetuning die veel efficiënter is dan volledige Finetuning - met vergelijkbare evaluatieresultaten zoals je spoedig zult zien.\n",
    "\n",
    "PEFT is een algemene term die Low-Rank Adaptation (LoRA) en prompt tuning omvat (wat NIET HETZELFDE is als prompt engineering!). In de meeste gevallen, wanneer iemand PEFT zegt, bedoelen ze doorgaans LoRA. LoRA stelt de gebruiker, op een zeer hoog niveau, in staat om hun model fijn te stellen met minder rekenbronnen (in sommige gevallen, een enkele GPU). Na Finetuning voor een specifieke taak, gebruikscase of tenant met LoRA, is het resultaat dat het originele LLM ongewijzigd blijft en er een nieuw getrainde \"LoRA-adapter\" tevoorschijn komt. Deze LoRA-adapter is veel, veel kleiner dan het originele LLM - in de orde van grootte van een enkelcijferig percentage van de grootte van het originele LLM (MB's versus GB's).\n",
    "\n",
    "Dat gezegd hebbende, moet de LoRA-adapter op het moment van inferentie herenigd en gecombineerd worden met zijn originele LLM om het inferentieverzoek te kunnen uitvoeren. Het voordeel is echter dat veel LoRA-adapters het originele LLM opnieuw kunnen gebruiken, wat het algehele geheugenvereisten vermindert bij het uitvoeren van meerdere taken en gebruikscases.\n",
    "\n",
    "<h3>Instellen van het PEFT/LoRA-model voor Fine-Tuning</h3>\n",
    "Je moet het PEFT/LoRA-model instellen voor fine-tuning met een nieuwe laag/parameteradapter. Met PEFT/LoRA bevries je het onderliggende LLM en train je alleen de adapter. Bekijk de onderstaande LoRA-configuratie. Let op de rang (r) hyperparameter, die de rang/dimensie van de te trainen adapter definieert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e871230a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aanmaken van een LoRA-configuratieobject. Deze configuratie bepaalt hoe de LoRA-aanpassing moet worden uitgevoerd.\n",
    "\n",
    "# Importeren van de benodigde klassen en functies uit de `peft` bibliotheek.\n",
    "# LoraConfig is een klasse die gebruikt wordt om de configuratie voor LoRA aanpassing te definiëren.\n",
    "# get_peft_model: Een functie om een model te verkrijgen dat is aangepast met de LoRA-configuratie.\n",
    "# TaskType: Een enumeratie die verschillende soorten taken definieert waarvoor het model kan worden geoptimaliseerd.\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "    \n",
    "# Hyperparameter: r\n",
    "# Klein (bijvoorbeeld 1 tot 16): Voor kleinere aanpassingen of wanneer beperkte rekenbronnen beschikbaar zijn. Geschikt voor minder complexe taken.\n",
    "# Middelgroot (bijvoorbeeld 17 tot 64): Een balans tussen aanpassingsvermogen en rekencomplexiteit. Vaak gebruikt voor gematigd complexe taken.\n",
    "# Groot (bijvoorbeeld 65 tot 128 of hoger): Voor complexe taken of wanneer het oorspronkelijke model zeer groot is. Vereist aanzienlijke rekenbronnen.\n",
    "\n",
    "# Hyperparameter: lora_alpha\n",
    "# Kleine waarden van lora_alpha: Wanneer lora_alpha klein is (bijvoorbeeld dichtbij 1), zijn de aanpassingen subtieler. Dit kan nuttig zijn in situaties waarin je het oorspronkelijke modelgedrag grotendeels wilt behouden en slechts kleine aanpassingen wilt maken.\n",
    "# Grotere waarden van lora_alpha: Een hogere waarde (zoals 10, 20 of hoger) maakt meer uitgesproken aanpassingen mogelijk. Dit kan wenselijk zijn wanneer het model aanzienlijk moet worden aangepast om te presteren op een specifieke taak of dataset.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8, # Rank: De rank van de LoRA-matrix. Een hogere rang kan leiden tot meer expressieve kracht, maar ook tot meer parameters.\n",
    "    lora_alpha=1, # LoRA Alpha: Een hyperparameter die de mate van aanpassing bepaalt.\n",
    "    target_modules=[\"q\", \"v\"], # Target Modules: Specificeert welke delen van het model moeten worden aangepast. 'q' (query) en 'v' (value) kunnen verwijzen naar specifieke componenten binnen het model.\n",
    "    lora_dropout=0.05, # LoRA Dropout: De dropout-rate toegepast op de LoRA-lagen, wat kan helpen om overfitting te voorkomen.\n",
    "    bias=\"none\", # Bias: Bepaalt hoe bias-termen worden behandeld in de LoRA-aanpassing. 'None' betekent dat er geen bias-termen worden gebruikt.\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # Task Type: Het type taak waarvoor het model wordt geoptimaliseerd. In dit geval is het een sequentie-naar-sequentie taalmodel (bijvoorbeeld FLAN-T5).\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9fe3d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(peft_model, lora_config)\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8af3e5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda GPU aanwezig: True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output_dir = f'./peft-dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "# Fine-tune the Flan-T5 model\n",
    "num_epochs = 8\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "cuda = torch.cuda.is_available()\n",
    "print(\"Cuda GPU aanwezig: \" + str(cuda))\n",
    "device = torch.device('cuda' if cuda else 'cpu')\n",
    "\n",
    "# Assume that `peft_model` is already defined and is your T5 model\n",
    "peft_model = peft_model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(peft_model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the data collator\n",
    "# The mlm=False argument indicates that the language model being used is not a Masked Language Model. \n",
    "# In masked language modeling, certain tokens in the input are randomly replaced with a mask token\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Since T5 is not a masked language model, we set this to False\n",
    ")\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,             # Higher learning rate than full fine-tuning.\n",
    "    overwrite_output_dir=True,      # Overwrite the content of the output directory\n",
    "    evaluation_strategy='epoch',    # Evaluate at the end of each epoch; kan de waarde 'epoch' of 'steps' hebben\n",
    "    num_train_epochs=num_epochs,    # Set the number of epochs\n",
    "    per_device_train_batch_size=10, # Batch size per device during training\n",
    "    save_steps=500,                 # After # steps model is saved \n",
    "    save_total_limit=2,             # Only last # models are saved. Older ones are deleted.\n",
    "    per_device_eval_batch_size=10,  # Batch size for evaluation\n",
    "    logging_dir='./logs',           # Directory for storing logs\n",
    "    logging_steps=5,                # Log every # steps\n",
    "\n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=trainingset,      # Pass the training dataset\n",
    "    eval_dataset=evalset,           # Pass the evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b386167c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='504' max='504' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [504/504 4:14:58, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.083500</td>\n",
       "      <td>0.014183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056200</td>\n",
       "      <td>0.006153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.004868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.004894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.004494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.004589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.004631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.004620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_trainer.train()\n",
    "\n",
    "peft_model_path=\"./peft-dialogue-summary-checkpoint-local\"\n",
    "\n",
    "#peft_trainer.model.save_pretrained(peft_model_path)\n",
    "#tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "959f619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document:\n",
      "Maak een samenvatting van de volgende tekst. ---- Wanneer u als bedrijf uit Nederland een aanvraag wilt doen voor een kentekenschorsing bij de Nederlandse Rijksoverheid, kunt u dit doen op basis van Artikel 50 van het Kentekenreglement. Een kentekenschorsing houdt in dat het kenteken van een voertuig tijdelijk wordt stopgezet, waardoor het voertuig niet op de openbare weg mag worden gebruikt. Dit kan bijvoorbeeld handig zijn wanneer u een voertuig voor langere tijd niet gebruikt, bijvoorbeeld tijdens een periode van stilstand of wanneer het voertuig wordt gerepareerd. Om een kentekenschorsing aan te vragen, dient u een online aanvraagformulier in te vullen op onze website. Hierbij dient u enkele gegevens in te vullen, zoals het kenteken van het voertuig, uw contactgegevens en de gewenste periode van schorsing. Na het indienen van de aanvraag ontvangt u een bevestiging per e-mail. De kosten voor een kentekenschorsing bedragen €73,10 per jaar. Na betaling van deze kosten wordt de schorsing van kracht. Het is belangrijk om te weten dat een kenteken\n",
      "\n",
      "Baseline menselijke samenvatting:\n",
      "Vraag een kentekenschorsing aan bij de Nederlandse Rijksoverheid op basis van Artikel 50 Kentekenreglement. Kosten bedragen €73,10 per jaar en de schorsing is maximaal 1 jaar geldig.\n",
      "\n",
      "Model gegenereerde samenvatting:\n",
      "Maak een samenvatting van de volgende tekst. ---- Wanneer u als bedrijf uit Nederland een aanvraag wilt doen voor een kentekenschorsing bij de Nederlandse Rijksoverheid, kunt u dit doen op basis van Artikel 50 van het Kentekenreglement. Een kentekenschorsing houdt in dat het kenteken van een voertuig tijdelijk wordt stopgezet, waardoor het voertuig niet op de openbare weg mag worden gebruikt. Dit kan bijvoorbeeld hand\n"
     ]
    }
   ],
   "source": [
    "ShowModelResult(peft_model, tokenizer, trainingset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5222e9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human_baseline_summaries</th>\n",
       "      <th>original_model_summaries</th>\n",
       "      <th>finetune_model_summaries</th>\n",
       "      <th>peft_model_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Om een sportactiviteitensubsidie aan te vragen...</td>\n",
       "      <td>De aanvraag kan om een tijd om een aanvraag om...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Als u in Nederland woont en lesgeld heeft beta...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vraag een vrijstelling aan voor een onderwijsa...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vraag een persoonsgebonden budget (PGB) aan bi...</td>\n",
       "      <td>PGB. ----</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Een bedrijf kan een afvalstortontheffing aanvr...</td>\n",
       "      <td>The province will be afvalstortontheffing on t...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Als een bedrijf uit Nederland zijn verplichtin...</td>\n",
       "      <td>De het het het het het het het het het het het...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Voor een verlenging van een duur prestatiebeur...</td>\n",
       "      <td>Maak een samenvatting van de volgende text. --...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Om een energiesubsidie aan te vragen bij uw ge...</td>\n",
       "      <td>Dutch will be able to make a decision on the a...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Voor het aanvragen van een monumentensubsidie ...</td>\n",
       "      <td>---- Um een subsidie aan te vinden voor een mo...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Maak een melding van een luchtvaart voorvalmel...</td>\n",
       "      <td>De voorval voorval voorval voorval voorval voo...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "      <td>Maak een samenvatting van de volgende tekst. -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            human_baseline_summaries  \\\n",
       "0  Om een sportactiviteitensubsidie aan te vragen...   \n",
       "1  Als u in Nederland woont en lesgeld heeft beta...   \n",
       "2  Vraag een vrijstelling aan voor een onderwijsa...   \n",
       "3  Vraag een persoonsgebonden budget (PGB) aan bi...   \n",
       "4  Een bedrijf kan een afvalstortontheffing aanvr...   \n",
       "5  Als een bedrijf uit Nederland zijn verplichtin...   \n",
       "6  Voor een verlenging van een duur prestatiebeur...   \n",
       "7  Om een energiesubsidie aan te vragen bij uw ge...   \n",
       "8  Voor het aanvragen van een monumentensubsidie ...   \n",
       "9  Maak een melding van een luchtvaart voorvalmel...   \n",
       "\n",
       "                            original_model_summaries  \\\n",
       "0  De aanvraag kan om een tijd om een aanvraag om...   \n",
       "1  De het het het het het het het het het het het...   \n",
       "2  De het het het het het het het het het het het...   \n",
       "3                                          PGB. ----   \n",
       "4  The province will be afvalstortontheffing on t...   \n",
       "5  De het het het het het het het het het het het...   \n",
       "6  Maak een samenvatting van de volgende text. --...   \n",
       "7  Dutch will be able to make a decision on the a...   \n",
       "8  ---- Um een subsidie aan te vinden voor een mo...   \n",
       "9  De voorval voorval voorval voorval voorval voo...   \n",
       "\n",
       "                            finetune_model_summaries  \\\n",
       "0  Maak een samenvatting van de volgende tekst. -...   \n",
       "1  Maak een samenvatting van de volgende tekst. -...   \n",
       "2  Maak een samenvatting van de volgende tekst. -...   \n",
       "3  Maak een samenvatting van de volgende tekst. -...   \n",
       "4  Maak een samenvatting van de volgende tekst. -...   \n",
       "5  Maak een samenvatting van de volgende tekst. -...   \n",
       "6  Maak een samenvatting van de volgende tekst. -...   \n",
       "7  Maak een samenvatting van de volgende tekst. -...   \n",
       "8  Maak een samenvatting van de volgende tekst. -...   \n",
       "9  Maak een samenvatting van de volgende tekst. -...   \n",
       "\n",
       "                                peft_model_summaries  \n",
       "0  Maak een samenvatting van de volgende tekst. -...  \n",
       "1  Maak een samenvatting van de volgende tekst. -...  \n",
       "2  Maak een samenvatting van de volgende tekst. -...  \n",
       "3  Maak een samenvatting van de volgende tekst. -...  \n",
       "4  Maak een samenvatting van de volgende tekst. -...  \n",
       "5  Maak een samenvatting van de volgende tekst. -...  \n",
       "6  Maak een samenvatting van de volgende tekst. -...  \n",
       "7  Maak een samenvatting van de volgende tekst. -...  \n",
       "8  Maak een samenvatting van de volgende tekst. -...  \n",
       "9  Maak een samenvatting van de volgende tekst. -...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (AI_Labs_dataset == False):\n",
    "    dialogues = evalset.dataset['document'][:10]\n",
    "    human_baseline_summaries = evalset.dataset['summary'][:10]\n",
    "else:\n",
    "    dialogues = evalset.dataframe['document'][:10]\n",
    "    human_baseline_summaries = evalset.dataframe['summary'][:10]\n",
    "\n",
    "original_model_summaries = []\n",
    "finetune_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = Makeprompt(dialogue)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids.to(device)\n",
    "\n",
    "    human_baseline_text_output = human_baseline_summaries\n",
    "    \n",
    "    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    finetune_model_outputs = finetune_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    finetune_model_text_output = tokenizer.decode(finetune_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    finetune_model_summaries.append(finetune_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, finetune_model_summaries, peft_model_summaries))\n",
    " \n",
    "df = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'finetune_model_summaries', 'peft_model_summaries'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "320b41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.1522105957661788, 'rouge2': 0.04607785902721354, 'rougeL': 0.10551464308540717, 'rougeLsum': 0.10651960098156801}\n",
      "FINETUNE MODEL:\n",
      "{'rouge1': 0.4838780248984317, 'rouge2': 0.29548220467517294, 'rougeL': 0.3691545863139732, 'rougeLsum': 0.36856419273572605}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.4838780248984317, 'rouge2': 0.29548220467517294, 'rougeL': 0.3691545863139732, 'rougeLsum': 0.36856419273572605}\n"
     ]
    }
   ],
   "source": [
    "human_baseline_summaries = list(human_baseline_summaries)\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "finetune_model_results = rouge.compute(\n",
    "    predictions=finetune_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(finetune_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print({\n",
    "    'rouge1': original_model_results['rouge1'],\n",
    "    'rouge2': original_model_results['rouge2'],\n",
    "    'rougeL': original_model_results['rougeL'],\n",
    "    'rougeLsum': original_model_results['rougeLsum'],\n",
    "})\n",
    "\n",
    "print('FINETUNE MODEL:')\n",
    "print({\n",
    "    'rouge1': finetune_model_results['rouge1'],\n",
    "    'rouge2': finetune_model_results['rouge2'],\n",
    "    'rougeL': finetune_model_results['rougeL'],\n",
    "    'rougeLsum': finetune_model_results['rougeLsum'],\n",
    "})\n",
    "print('PEFT MODEL:')\n",
    "print({\n",
    "    'rouge1': peft_model_results['rouge1'],\n",
    "    'rouge2': peft_model_results['rouge2'],\n",
    "    'rougeL': peft_model_results['rougeL'],\n",
    "    'rougeLsum': peft_model_results['rougeLsum'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "033c77bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\n",
      "rouge1: 217.90%\n",
      "rouge2: 541.27%\n",
      "rougeL: 249.86%\n",
      "rougeLsum: 246.01%\n"
     ]
    }
   ],
   "source": [
    "#Calculate the improvement of PEFT over the original model:\n",
    "print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n",
    "\n",
    "# Extract the mid fmeasure values for both models\n",
    "original_scores = {\n",
    "    'rouge1': original_model_results['rouge1'],\n",
    "    'rouge2': original_model_results['rouge2'],\n",
    "    'rougeL': original_model_results['rougeL'],\n",
    "    'rougeLsum': original_model_results['rougeLsum'],\n",
    "}\n",
    "\n",
    "peft_scores = {\n",
    "    'rouge1': peft_model_results['rouge1'],\n",
    "    'rouge2': peft_model_results['rouge2'],\n",
    "    'rougeL': peft_model_results['rougeL'],\n",
    "    'rougeLsum': peft_model_results['rougeLsum'],\n",
    "}\n",
    "\n",
    "# Calculate the percentage improvement for each score\n",
    "for key in original_scores:\n",
    "    original_score = original_scores[key]\n",
    "    peft_score = peft_scores[key]\n",
    "    improvement = (peft_score - original_score) / original_score * 100\n",
    "    print(f'{key}: {improvement:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129a90a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
